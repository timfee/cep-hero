{
  "documents": {
    "docs_ragas_io_en_stable_.md": {
      "url": "https://docs.ragas.io/en/stable/",
      "title": "Ragas",
      "fetchedAt": "2026-02-01T19:26:30.712Z",
      "expiresAt": "2026-02-02T19:26:30.712Z",
      "size": 2688
    },
    "docs_confident-ai_com_docs_deepeval-introduction.md": {
      "url": "https://docs.confident-ai.com/docs/deepeval-introduction",
      "title": "DeepEval by Confident AI - The LLM Evaluation Framework",
      "fetchedAt": "2026-02-01T19:26:31.137Z",
      "expiresAt": "2026-02-02T19:26:31.137Z",
      "size": 179
    },
    "github_com_openai_evals.md": {
      "url": "https://github.com/openai/evals",
      "title": "GitHub - openai/evals: Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.",
      "fetchedAt": "2026-02-01T19:26:31.319Z",
      "expiresAt": "2026-02-02T19:26:31.319Z",
      "size": 7637
    }
  },
  "lastUpdated": "2026-02-01T19:26:31.322Z"
}